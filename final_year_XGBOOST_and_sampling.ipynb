{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPNZ3IeoPZaDCRn9+o/yFzO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AshvinVignesh/Final_year/blob/main/final_year_XGBOOST_and_sampling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Wag7ZdrgN_5"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import GridSearchCV, StratifiedKFold, cross_validate\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import os\n",
        "\n",
        "def custom_tokenizer(text):\n",
        "    return text.split()\n",
        "\n",
        "def save_models(models, filename=\"final_models.pkl\"):\n",
        "    \"\"\"Save trained models and label encoders to a pickle file.\"\"\"\n",
        "    with open(filename, 'wb') as f:\n",
        "        pickle.dump(models, f)\n",
        "    print(f\"Models saved to {filename}\")\n",
        "\n",
        "def load_models(filename=\"final_models.pkl\"):\n",
        "    \"\"\"Load trained models and label encoders from a pickle file.\"\"\"\n",
        "    if not os.path.exists(filename):\n",
        "        raise FileNotFoundError(f\"The file {filename} does not exist.\")\n",
        "    with open(filename, 'rb') as f:\n",
        "        models = pickle.load(f)\n",
        "    print(f\"Models loaded from {filename}\")\n",
        "    return models\n",
        "\n",
        "def predict_all_aspects_from_file(sentence, model_filename=\"final_models.pkl\"):\n",
        "    \"\"\"Load the saved models and predict aspects for a given sentence.\"\"\"\n",
        "    models = load_models(model_filename)\n",
        "    predictions = {}\n",
        "\n",
        "    for aspect, model_info in models.items():\n",
        "        model = model_info['model']\n",
        "        le = model_info['label_encoder']\n",
        "        pred_encoded = model.predict([sentence])[0]\n",
        "        pred_label = le.inverse_transform([pred_encoded])[0]\n",
        "        predictions[aspect] = pred_label\n",
        "\n",
        "    return predictions"
      ],
      "metadata": {
        "id": "J4mE-Sc0FS8n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_data(df, aspect_column):\n",
        "\n",
        "    X = df[['clean_text']]\n",
        "    y = df[aspect_column]\n",
        "    return X, y"
      ],
      "metadata": {
        "id": "hplrNeM5hHJs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_data(df, aspect_column):\n",
        "\n",
        "    X = df[['R_clean_text']]\n",
        "    y = df[aspect_column]\n",
        "    return X, y"
      ],
      "metadata": {
        "id": "pVTzaibihBP1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "train_df = pd.read_csv(\"train_data.csv\")\n",
        "test_df = pd.read_csv(\"test_data.csv\")\n",
        "\n",
        "def train_final_models_grid_search(train_df, test_df, aspects):\n",
        "    \"\"\"\n",
        "    For each aspect:\n",
        "    1. Perform grid search on the training data to find the best parameters.\n",
        "    2. Using the best parameters, perform 10-fold cross-validation,\n",
        "       returning both the average accuracy and the list of fold estimators.\n",
        "    3. Select the best estimator from CV (highest test score) and evaluate it on test data.\n",
        "    Returns the best models along with their label encoders and classification reports.\n",
        "    \"\"\"\n",
        "    best_models = {}\n",
        "    train_reports = {}\n",
        "    test_reports = {}\n",
        "\n",
        "    # Parameter grid for the XGBoost classifier\n",
        "    param_grid = {\n",
        "        'clf__learning_rate': [0.01, 0.1, 0.2],\n",
        "        'clf__max_depth': [3, 5, 7],\n",
        "        'clf__n_estimators': [100, 200, 300],\n",
        "        'clf__subsample': [0.8, 1.0]\n",
        "    }\n",
        "\n",
        "    for aspect in aspects:\n",
        "        print(f\"\\n=== Processing aspect: {aspect} ===\")\n",
        "        # Prepare data for current aspect\n",
        "        X_train, y_train = prepare_data(train_df, aspect)\n",
        "        X_test, y_test = prepare_data(test_df, aspect)\n",
        "\n",
        "        # Encode target labels to numerical values\n",
        "        le = LabelEncoder()\n",
        "        y_train_encoded = le.fit_transform(y_train)\n",
        "        y_test_encoded = le.fit_transform(y_test)\n",
        "\n",
        "        # Build initial pipeline with XGBoost classifier\n",
        "        pipeline = Pipeline([\n",
        "            ('tfidf', TfidfVectorizer(max_features=5000,\n",
        "                                        ngram_range=(1, 2),\n",
        "                                        min_df=5,\n",
        "                                        max_df=0.90,\n",
        "                                        tokenizer=custom_tokenizer)),\n",
        "            ('clf', XGBClassifier(random_state=42,\n",
        "                                  use_label_encoder=False,\n",
        "                                  eval_metric=\"logloss\"))\n",
        "        ])\n",
        "\n",
        "        # 10-fold CV strategy for grid search\n",
        "        cv_strategy = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
        "\n",
        "        # Ensure there are no missing or empty texts in training set\n",
        "        X_train = X_train.dropna(subset=['clean_text'])\n",
        "        X_train = X_train[X_train['clean_text'].str.strip() != '']\n",
        "\n",
        "        # --- Step 1: Grid Search ---\n",
        "        grid = GridSearchCV(pipeline, param_grid, cv=cv_strategy,\n",
        "                            scoring='balanced_accuracy', n_jobs=-1, return_train_score=True)\n",
        "        grid.fit(X_train['clean_text'], y_train_encoded)\n",
        "        best_params = grid.best_params_\n",
        "        print(f\"Best parameters for aspect '{aspect}': {best_params}\")\n",
        "\n",
        "        # --- Step 2: Build a new pipeline using the best parameters ---\n",
        "        best_pipeline = Pipeline([\n",
        "            ('tfidf', TfidfVectorizer(max_features=5000,\n",
        "                                        ngram_range=(1, 2),\n",
        "                                        min_df=5,\n",
        "                                        max_df=0.90,\n",
        "                                        tokenizer=custom_tokenizer)),\n",
        "            ('clf', XGBClassifier(random_state=42,\n",
        "                                  use_label_encoder=False,\n",
        "                                  eval_metric=\"logloss\",\n",
        "                                  learning_rate=best_params['clf__learning_rate'],\n",
        "                                  max_depth=best_params['clf__max_depth'],\n",
        "                                  n_estimators=best_params['clf__n_estimators'],\n",
        "                                  subsample=best_params['clf__subsample']))\n",
        "        ])\n",
        "\n",
        "        # --- Step 3: Perform 10-Fold Cross Validation with estimator return ---\n",
        "        cv_results = cross_validate(best_pipeline, X_train['clean_text'], y_train_encoded,\n",
        "                                    cv=StratifiedKFold(n_splits=10, shuffle=True, random_state=42),\n",
        "                                    scoring='balanced_accuracy', return_train_score=True,\n",
        "                                    return_estimator=True, n_jobs=-1)\n",
        "        avg_cv_score_test = cv_results['test_score'].mean()\n",
        "        avg_cv_score_train = cv_results['train_score'].mean()\n",
        "        print(f\"Average 10-fold CV test Accuracy for aspect '{aspect}': {avg_cv_score_test:.4f}\")\n",
        "        print(f\"Average 10-fold CV train Accuracy for aspect '{aspect}': {avg_cv_score_train:.4f}\")\n",
        "\n",
        "        # --- Step 4: Select the best estimator from the CV folds ---\n",
        "        best_index = np.argmax(cv_results['test_score'])\n",
        "        best_cv_estimator = cv_results['estimator'][best_index]\n",
        "\n",
        "        # Evaluate the selected estimator on the training data (optional)\n",
        "        y_train_pred = best_cv_estimator.predict(X_train['clean_text'])\n",
        "        train_acc = accuracy_score(y_train_encoded, y_train_pred)\n",
        "        print(\"Final Training Accuracy: {:.4f}\".format(train_acc))\n",
        "        train_report = classification_report(y_train_encoded, y_train_pred)\n",
        "        print(\"Final Training Classification Report:\\n\", train_report)\n",
        "\n",
        "        # --- Step 5: Evaluate on Test Data ---\n",
        "        y_test_pred = best_cv_estimator.predict(X_test['clean_text'])\n",
        "        test_acc = accuracy_score(y_test_encoded, y_test_pred)\n",
        "        print(\"Test Accuracy: {:.4f}\".format(test_acc))\n",
        "        test_report = classification_report(y_test_encoded, y_test_pred)\n",
        "        print(\"Test Classification Report:\\n\", test_report)\n",
        "\n",
        "        # Save results for the current aspect\n",
        "        best_models[aspect.lower()] = {\n",
        "            \"model\": best_cv_estimator,\n",
        "            \"label_encoder\": le\n",
        "        }\n",
        "        train_reports[aspect.lower()] = train_report\n",
        "        test_reports[aspect.lower()] = test_report\n",
        "\n",
        "    return best_models, train_reports, test_reports"
      ],
      "metadata": {
        "id": "jkQvZ2-_gZQc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "aspects = ['Acting', 'direction', 'Music','ovr_sent']"
      ],
      "metadata": {
        "id": "YUOG6pAehKVj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_models, train_reports, test_reports = train_final_models_grid_search(train_df,test_df, aspects)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3PlkyXMFhObc",
        "outputId": "3d14fb2d-7725-415e-d70a-361903fda708"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Processing aspect: Acting ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [04:03:39] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters for aspect 'Acting': {'clf__learning_rate': 0.2, 'clf__max_depth': 5, 'clf__n_estimators': 300, 'clf__subsample': 0.8}\n",
            "Average 10-fold CV test Accuracy for aspect 'Acting': 0.5464\n",
            "Average 10-fold CV train Accuracy for aspect 'Acting': 0.8060\n",
            "Final Training Accuracy: 0.9312\n",
            "Final Training Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.56      0.71       131\n",
            "           1       0.93      0.99      0.96      2962\n",
            "           2       0.93      0.79      0.85       848\n",
            "\n",
            "    accuracy                           0.93      3941\n",
            "   macro avg       0.95      0.78      0.84      3941\n",
            "weighted avg       0.93      0.93      0.93      3941\n",
            "\n",
            "Test Accuracy: 0.8529\n",
            "Test Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.09      0.03      0.04        34\n",
            "           1       0.88      0.96      0.92       746\n",
            "           2       0.77      0.61      0.68       206\n",
            "\n",
            "    accuracy                           0.85       986\n",
            "   macro avg       0.58      0.53      0.55       986\n",
            "weighted avg       0.83      0.85      0.84       986\n",
            "\n",
            "\n",
            "=== Processing aspect: direction ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [04:33:36] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters for aspect 'direction': {'clf__learning_rate': 0.2, 'clf__max_depth': 7, 'clf__n_estimators': 300, 'clf__subsample': 0.8}\n",
            "Average 10-fold CV test Accuracy for aspect 'direction': 0.5178\n",
            "Average 10-fold CV train Accuracy for aspect 'direction': 0.8727\n",
            "Final Training Accuracy: 0.9437\n",
            "Final Training Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.76      0.84       568\n",
            "           1       0.94      0.99      0.97      3128\n",
            "           2       0.93      0.76      0.84       245\n",
            "\n",
            "    accuracy                           0.94      3941\n",
            "   macro avg       0.94      0.84      0.88      3941\n",
            "weighted avg       0.94      0.94      0.94      3941\n",
            "\n",
            "Test Accuracy: 0.8124\n",
            "Test Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.38      0.27      0.32       124\n",
            "           1       0.87      0.93      0.90       804\n",
            "           2       0.53      0.34      0.42        58\n",
            "\n",
            "    accuracy                           0.81       986\n",
            "   macro avg       0.59      0.52      0.54       986\n",
            "weighted avg       0.79      0.81      0.80       986\n",
            "\n",
            "\n",
            "=== Processing aspect: Music ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [04:57:54] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters for aspect 'Music': {'clf__learning_rate': 0.2, 'clf__max_depth': 3, 'clf__n_estimators': 300, 'clf__subsample': 1.0}\n",
            "Average 10-fold CV test Accuracy for aspect 'Music': 0.6700\n",
            "Average 10-fold CV train Accuracy for aspect 'Music': 0.8664\n",
            "Final Training Accuracy: 0.9772\n",
            "Final Training Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.74      0.84        89\n",
            "           1       0.98      1.00      0.99      3446\n",
            "           2       0.98      0.84      0.90       406\n",
            "\n",
            "    accuracy                           0.98      3941\n",
            "   macro avg       0.97      0.86      0.91      3941\n",
            "weighted avg       0.98      0.98      0.98      3941\n",
            "\n",
            "Test Accuracy: 0.9544\n",
            "Test Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.50      0.17      0.25        18\n",
            "           1       0.97      0.99      0.98       867\n",
            "           2       0.80      0.77      0.79       101\n",
            "\n",
            "    accuracy                           0.95       986\n",
            "   macro avg       0.76      0.64      0.67       986\n",
            "weighted avg       0.95      0.95      0.95       986\n",
            "\n",
            "\n",
            "=== Processing aspect: ovr_sent ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [05:29:15] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters for aspect 'ovr_sent': {'clf__learning_rate': 0.2, 'clf__max_depth': 7, 'clf__n_estimators': 300, 'clf__subsample': 0.8}\n",
            "Average 10-fold CV test Accuracy for aspect 'ovr_sent': 0.5622\n",
            "Average 10-fold CV train Accuracy for aspect 'ovr_sent': 0.8215\n",
            "Final Training Accuracy: 0.8622\n",
            "Final Training Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.86      0.87      1346\n",
            "           1       0.88      0.60      0.71       567\n",
            "           2       0.85      0.94      0.89      2028\n",
            "\n",
            "    accuracy                           0.86      3941\n",
            "   macro avg       0.87      0.80      0.82      3941\n",
            "weighted avg       0.86      0.86      0.86      3941\n",
            "\n",
            "Test Accuracy: 0.6846\n",
            "Test Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.68      0.63      0.65       348\n",
            "           1       0.37      0.18      0.24       139\n",
            "           2       0.72      0.86      0.79       499\n",
            "\n",
            "    accuracy                           0.68       986\n",
            "   macro avg       0.59      0.56      0.56       986\n",
            "weighted avg       0.66      0.68      0.66       986\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_models"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tV1I-PmMF_KM",
        "outputId": "08c91665-46dc-4b36-a313-ff122e5e3bdc",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'acting': {'model': Pipeline(steps=[('tfidf',\n",
              "                   TfidfVectorizer(max_df=0.9, max_features=5000, min_df=5,\n",
              "                                   ngram_range=(1, 2),\n",
              "                                   tokenizer=<function custom_tokenizer at 0x7f81f8fd7a60>)),\n",
              "                  ('clf',\n",
              "                   XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
              "                                 colsample_bylevel=None, colsample_bynode=None,\n",
              "                                 colsample_bytree=None, device=None,\n",
              "                                 early_stopping_rounds=None,\n",
              "                                 enable_categorical=False, ev...\n",
              "                                 feature_types=None, gamma=None, grow_policy=None,\n",
              "                                 importance_type=None,\n",
              "                                 interaction_constraints=None, learning_rate=0.2,\n",
              "                                 max_bin=None, max_cat_threshold=None,\n",
              "                                 max_cat_to_onehot=None, max_delta_step=None,\n",
              "                                 max_depth=5, max_leaves=None,\n",
              "                                 min_child_weight=None, missing=nan,\n",
              "                                 monotone_constraints=None, multi_strategy=None,\n",
              "                                 n_estimators=300, n_jobs=None,\n",
              "                                 num_parallel_tree=None,\n",
              "                                 objective='multi:softprob', ...))]),\n",
              "  'label_encoder': LabelEncoder()},\n",
              " 'direction': {'model': Pipeline(steps=[('tfidf',\n",
              "                   TfidfVectorizer(max_df=0.9, max_features=5000, min_df=5,\n",
              "                                   ngram_range=(1, 2),\n",
              "                                   tokenizer=<function custom_tokenizer at 0x7f81f89c4180>)),\n",
              "                  ('clf',\n",
              "                   XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
              "                                 colsample_bylevel=None, colsample_bynode=None,\n",
              "                                 colsample_bytree=None, device=None,\n",
              "                                 early_stopping_rounds=None,\n",
              "                                 enable_categorical=False, ev...\n",
              "                                 feature_types=None, gamma=None, grow_policy=None,\n",
              "                                 importance_type=None,\n",
              "                                 interaction_constraints=None, learning_rate=0.2,\n",
              "                                 max_bin=None, max_cat_threshold=None,\n",
              "                                 max_cat_to_onehot=None, max_delta_step=None,\n",
              "                                 max_depth=7, max_leaves=None,\n",
              "                                 min_child_weight=None, missing=nan,\n",
              "                                 monotone_constraints=None, multi_strategy=None,\n",
              "                                 n_estimators=300, n_jobs=None,\n",
              "                                 num_parallel_tree=None,\n",
              "                                 objective='multi:softprob', ...))]),\n",
              "  'label_encoder': LabelEncoder()},\n",
              " 'music': {'model': Pipeline(steps=[('tfidf',\n",
              "                   TfidfVectorizer(max_df=0.9, max_features=5000, min_df=5,\n",
              "                                   ngram_range=(1, 2),\n",
              "                                   tokenizer=<function custom_tokenizer at 0x7f81f89c4720>)),\n",
              "                  ('clf',\n",
              "                   XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
              "                                 colsample_bylevel=None, colsample_bynode=None,\n",
              "                                 colsample_bytree=None, device=None,\n",
              "                                 early_stopping_rounds=None,\n",
              "                                 enable_categorical=False, ev...\n",
              "                                 feature_types=None, gamma=None, grow_policy=None,\n",
              "                                 importance_type=None,\n",
              "                                 interaction_constraints=None, learning_rate=0.2,\n",
              "                                 max_bin=None, max_cat_threshold=None,\n",
              "                                 max_cat_to_onehot=None, max_delta_step=None,\n",
              "                                 max_depth=3, max_leaves=None,\n",
              "                                 min_child_weight=None, missing=nan,\n",
              "                                 monotone_constraints=None, multi_strategy=None,\n",
              "                                 n_estimators=300, n_jobs=None,\n",
              "                                 num_parallel_tree=None,\n",
              "                                 objective='multi:softprob', ...))]),\n",
              "  'label_encoder': LabelEncoder()},\n",
              " 'ovr_sent': {'model': Pipeline(steps=[('tfidf',\n",
              "                   TfidfVectorizer(max_df=0.9, max_features=5000, min_df=5,\n",
              "                                   ngram_range=(1, 2),\n",
              "                                   tokenizer=<function custom_tokenizer at 0x7f81f89c45e0>)),\n",
              "                  ('clf',\n",
              "                   XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
              "                                 colsample_bylevel=None, colsample_bynode=None,\n",
              "                                 colsample_bytree=None, device=None,\n",
              "                                 early_stopping_rounds=None,\n",
              "                                 enable_categorical=False, ev...\n",
              "                                 feature_types=None, gamma=None, grow_policy=None,\n",
              "                                 importance_type=None,\n",
              "                                 interaction_constraints=None, learning_rate=0.2,\n",
              "                                 max_bin=None, max_cat_threshold=None,\n",
              "                                 max_cat_to_onehot=None, max_delta_step=None,\n",
              "                                 max_depth=7, max_leaves=None,\n",
              "                                 min_child_weight=None, missing=nan,\n",
              "                                 monotone_constraints=None, multi_strategy=None,\n",
              "                                 n_estimators=300, n_jobs=None,\n",
              "                                 num_parallel_tree=None,\n",
              "                                 objective='multi:softprob', ...))]),\n",
              "  'label_encoder': LabelEncoder()}}"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "save_models(best_models)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 277
        },
        "id": "cP1pHs6hFPEp",
        "outputId": "68a155a3-6642-4c11-ef8d-86ddae7cccc9",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "PicklingError",
          "evalue": "Can't pickle <function custom_tokenizer at 0x7f81f8fd7a60>: it's not the same object as __main__.custom_tokenizer",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPicklingError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-bdd9fc314fd6>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msave_models\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_models\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-10-2921d71b0a14>\u001b[0m in \u001b[0;36msave_models\u001b[0;34m(models, filename)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;34m\"\"\"Save trained models and label encoders to a pickle file.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Models saved to {filename}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mPicklingError\u001b[0m: Can't pickle <function custom_tokenizer at 0x7f81f8fd7a60>: it's not the same object as __main__.custom_tokenizer"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming custom_tokenizer and prepare_data are defined elsewhere\n",
        "train_df = pd.read_csv(\"Romanised_train_data.csv\")\n",
        "test_df = pd.read_csv(\"Romanised_test_data.csv\")\n",
        "\n",
        "def train_final_models_grid_search(train_df, test_df, aspects):\n",
        "    \"\"\"\n",
        "    For each aspect:\n",
        "    1. Perform grid search on the training data to find the best parameters.\n",
        "    2. Using the best parameters, perform 10-fold cross-validation,\n",
        "       returning both the average accuracy and the list of fold estimators.\n",
        "    3. Select the best estimator from CV (highest test score) and evaluate it on test data.\n",
        "    Returns the best models along with their label encoders and classification reports.\n",
        "    \"\"\"\n",
        "    best_models = {}\n",
        "    train_reports = {}\n",
        "    test_reports = {}\n",
        "\n",
        "    # Parameter grid for the XGBoost classifier\n",
        "    param_grid = {\n",
        "        'clf__learning_rate': [0.01, 0.1, 0.2],\n",
        "        'clf__max_depth': [3, 5, 7],\n",
        "        'clf__n_estimators': [100, 200, 300],\n",
        "        'clf__subsample': [0.8, 1.0]\n",
        "    }\n",
        "\n",
        "    for aspect in aspects:\n",
        "        print(f\"\\n=== Processing aspect: {aspect} ===\")\n",
        "        # Prepare data for current aspect\n",
        "        X_train, y_train = prepare_data(train_df, aspect)\n",
        "        X_test, y_test = prepare_data(test_df, aspect)\n",
        "\n",
        "        # Encode target labels to numerical values\n",
        "        le = LabelEncoder()\n",
        "        y_train_encoded = le.fit_transform(y_train)\n",
        "        y_test_encoded = le.fit_transform(y_test)\n",
        "\n",
        "        # Build initial pipeline with XGBoost classifier\n",
        "        pipeline = Pipeline([\n",
        "            ('tfidf', TfidfVectorizer(max_features=5000,\n",
        "                                        ngram_range=(1, 2),\n",
        "                                        min_df=5,\n",
        "                                        max_df=0.90,\n",
        "                                        tokenizer=custom_tokenizer)),\n",
        "            ('clf', XGBClassifier(random_state=42,\n",
        "                                  use_label_encoder=False,\n",
        "                                  eval_metric=\"logloss\"))\n",
        "        ])\n",
        "\n",
        "        # 10-fold CV strategy for grid search\n",
        "        cv_strategy = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
        "\n",
        "        # Ensure there are no missing or empty texts in training set\n",
        "        X_train = X_train.dropna(subset=['R_clean_text'])\n",
        "        X_train = X_train[X_train['R_clean_text'].str.strip() != '']\n",
        "\n",
        "        # --- Step 1: Grid Search ---\n",
        "        grid = GridSearchCV(pipeline, param_grid, cv=cv_strategy,\n",
        "                            scoring='balanced_accuracy', n_jobs=-1, return_train_score=True)\n",
        "        grid.fit(X_train['R_clean_text'], y_train_encoded)\n",
        "        best_params = grid.best_params_\n",
        "        print(f\"Best parameters for aspect '{aspect}': {best_params}\")\n",
        "\n",
        "        # --- Step 2: Build a new pipeline using the best parameters ---\n",
        "        best_pipeline = Pipeline([\n",
        "            ('tfidf', TfidfVectorizer(max_features=5000,\n",
        "                                        ngram_range=(1, 2),\n",
        "                                        min_df=5,\n",
        "                                        max_df=0.90,\n",
        "                                        tokenizer=custom_tokenizer)),\n",
        "            ('clf', XGBClassifier(random_state=42,\n",
        "                                  use_label_encoder=False,\n",
        "                                  eval_metric=\"logloss\",\n",
        "                                  learning_rate=best_params['clf__learning_rate'],\n",
        "                                  max_depth=best_params['clf__max_depth'],\n",
        "                                  n_estimators=best_params['clf__n_estimators'],\n",
        "                                  subsample=best_params['clf__subsample']))\n",
        "        ])\n",
        "\n",
        "        # --- Step 3: Perform 10-Fold Cross Validation with estimator return ---\n",
        "        cv_results = cross_validate(best_pipeline, X_train['R_clean_text'], y_train_encoded,\n",
        "                                    cv=StratifiedKFold(n_splits=10, shuffle=True, random_state=42),\n",
        "                                    scoring='balanced_accuracy', return_train_score=True,\n",
        "                                    return_estimator=True, n_jobs=-1)\n",
        "        avg_cv_score_test = cv_results['test_score'].mean()\n",
        "        avg_cv_score_train = cv_results['train_score'].mean()\n",
        "        print(f\"Average 10-fold CV test Accuracy for aspect '{aspect}': {avg_cv_score_test:.4f}\")\n",
        "        print(f\"Average 10-fold CV train Accuracy for aspect '{aspect}': {avg_cv_score_train:.4f}\")\n",
        "\n",
        "        # --- Step 4: Select the best estimator from the CV folds ---\n",
        "        best_index = np.argmax(cv_results['test_score'])\n",
        "        best_cv_estimator = cv_results['estimator'][best_index]\n",
        "\n",
        "        # Evaluate the selected estimator on the training data (optional)\n",
        "        y_train_pred = best_cv_estimator.predict(X_train['R_clean_text'])\n",
        "        train_acc = accuracy_score(y_train_encoded, y_train_pred)\n",
        "        print(\"Final Training Accuracy: {:.4f}\".format(train_acc))\n",
        "        train_report = classification_report(y_train_encoded, y_train_pred)\n",
        "        print(\"Final Training Classification Report:\\n\", train_report)\n",
        "\n",
        "        # --- Step 5: Evaluate on Test Data ---\n",
        "        y_test_pred = best_cv_estimator.predict(X_test['R_clean_text'])\n",
        "        test_acc = accuracy_score(y_test_encoded, y_test_pred)\n",
        "        print(\"Test Accuracy: {:.4f}\".format(test_acc))\n",
        "        test_report = classification_report(y_test_encoded, y_test_pred)\n",
        "        print(\"Test Classification Report:\\n\", test_report)\n",
        "\n",
        "        # Save results for the current aspect\n",
        "        best_models[aspect.lower()] = {\n",
        "            \"model\": best_cv_estimator,\n",
        "            \"label_encoder\": le\n",
        "        }\n",
        "        train_reports[aspect.lower()] = train_report\n",
        "        test_reports[aspect.lower()] = test_report\n",
        "\n",
        "    return best_models, train_reports, test_reports"
      ],
      "metadata": {
        "id": "3gfAziqPjIHE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_models, train_reports, test_reports = train_final_models_grid_search(train_df,test_df, aspects)"
      ],
      "metadata": {
        "id": "WylDoSQ-ke3s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc2ad7e5-e7d5-4a16-8eb7-674e5e62def6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Processing aspect: Acting ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [08:39:18] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters for aspect 'Acting': {'clf__learning_rate': 0.2, 'clf__max_depth': 5, 'clf__n_estimators': 300, 'clf__subsample': 1.0}\n",
            "Average 10-fold CV test Accuracy for aspect 'Acting': 0.5611\n",
            "Average 10-fold CV train Accuracy for aspect 'Acting': 0.8193\n",
            "Final Training Accuracy: 0.9386\n",
            "Final Training Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.60      0.75       131\n",
            "           1       0.93      0.99      0.96      2962\n",
            "           2       0.96      0.80      0.87       848\n",
            "\n",
            "    accuracy                           0.94      3941\n",
            "   macro avg       0.96      0.80      0.86      3941\n",
            "weighted avg       0.94      0.94      0.94      3941\n",
            "\n",
            "Test Accuracy: 0.8580\n",
            "Test Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.41      0.21      0.27        34\n",
            "           1       0.88      0.96      0.92       746\n",
            "           2       0.78      0.61      0.69       206\n",
            "\n",
            "    accuracy                           0.86       986\n",
            "   macro avg       0.69      0.59      0.63       986\n",
            "weighted avg       0.85      0.86      0.85       986\n",
            "\n",
            "\n",
            "=== Processing aspect: direction ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [09:10:30] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters for aspect 'direction': {'clf__learning_rate': 0.2, 'clf__max_depth': 7, 'clf__n_estimators': 300, 'clf__subsample': 1.0}\n",
            "Average 10-fold CV test Accuracy for aspect 'direction': 0.5367\n",
            "Average 10-fold CV train Accuracy for aspect 'direction': 0.8699\n",
            "Final Training Accuracy: 0.9465\n",
            "Final Training Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.76      0.85       568\n",
            "           1       0.94      1.00      0.97      3128\n",
            "           2       0.95      0.77      0.85       245\n",
            "\n",
            "    accuracy                           0.95      3941\n",
            "   macro avg       0.95      0.84      0.89      3941\n",
            "weighted avg       0.95      0.95      0.94      3941\n",
            "\n",
            "Test Accuracy: 0.8367\n",
            "Test Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.51      0.32      0.39       124\n",
            "           1       0.88      0.95      0.91       804\n",
            "           2       0.60      0.43      0.50        58\n",
            "\n",
            "    accuracy                           0.84       986\n",
            "   macro avg       0.66      0.57      0.60       986\n",
            "weighted avg       0.82      0.84      0.82       986\n",
            "\n",
            "\n",
            "=== Processing aspect: Music ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [09:35:30] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters for aspect 'Music': {'clf__learning_rate': 0.2, 'clf__max_depth': 7, 'clf__n_estimators': 200, 'clf__subsample': 1.0}\n",
            "Average 10-fold CV test Accuracy for aspect 'Music': 0.6561\n",
            "Average 10-fold CV train Accuracy for aspect 'Music': 0.9186\n",
            "Final Training Accuracy: 0.9838\n",
            "Final Training Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.80      0.88        89\n",
            "           1       0.99      1.00      0.99      3446\n",
            "           2       0.97      0.89      0.93       406\n",
            "\n",
            "    accuracy                           0.98      3941\n",
            "   macro avg       0.98      0.90      0.93      3941\n",
            "weighted avg       0.98      0.98      0.98      3941\n",
            "\n",
            "Test Accuracy: 0.9604\n",
            "Test Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.28      0.42        18\n",
            "           1       0.98      0.99      0.98       867\n",
            "           2       0.81      0.83      0.82       101\n",
            "\n",
            "    accuracy                           0.96       986\n",
            "   macro avg       0.87      0.70      0.74       986\n",
            "weighted avg       0.96      0.96      0.96       986\n",
            "\n",
            "\n",
            "=== Processing aspect: ovr_sent ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [10:08:03] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters for aspect 'ovr_sent': {'clf__learning_rate': 0.2, 'clf__max_depth': 7, 'clf__n_estimators': 300, 'clf__subsample': 1.0}\n",
            "Average 10-fold CV test Accuracy for aspect 'ovr_sent': 0.5808\n",
            "Average 10-fold CV train Accuracy for aspect 'ovr_sent': 0.8567\n",
            "Final Training Accuracy: 0.8886\n",
            "Final Training Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.89      0.89      1346\n",
            "           1       0.93      0.64      0.76       567\n",
            "           2       0.88      0.96      0.92      2028\n",
            "\n",
            "    accuracy                           0.89      3941\n",
            "   macro avg       0.90      0.83      0.86      3941\n",
            "weighted avg       0.89      0.89      0.88      3941\n",
            "\n",
            "Test Accuracy: 0.7079\n",
            "Test Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.72      0.68      0.70       348\n",
            "           1       0.50      0.23      0.32       139\n",
            "           2       0.73      0.86      0.79       499\n",
            "\n",
            "    accuracy                           0.71       986\n",
            "   macro avg       0.65      0.59      0.60       986\n",
            "weighted avg       0.69      0.71      0.69       986\n",
            "\n"
          ]
        }
      ]
    }
  ]
}